## ‚ö†Ô∏è Configure Alertmanager
- Review the Alertmanager configuration files located in `day-4/alerts-alertmanager-servicemonitor-manifest` but below is the brief overview
    - Before configuring Alertmanager, we need credentials to send emails. For this project, we are using Gmail, but any SMTP provider like AWS SES can be used. so please grab the credentials for that.
    - Open your Google account settings and search App password & create a new password & put the password in `day-4/alerts-alertmanager-servicemonitor-manifest/email-secret.yml`
    - One last thing, please add your email id in the `day-4/alerts-alertmanager-servicemonitor-manifest/alertmanagerconfig.yml`
- **HighCpuUsage**: Triggers a warning alert if the average CPU usage across instances exceeds 50% for more than 5 minutes.
- **PodRestart**: Triggers a critical alert immediately if any pod restarts more than 2 times.
- Apply the manifest files to your cluster by running:
```bash
kubectl apply -k alerts-alertmanager-servicemonitor-manifest/
```
- Wait for 4-5 minutes and then check the Prometheus UI to confirm that the custom metrics implemented in the Node.js application are available:
    - `http_requests_total`: counter
    - `http_request_duration_seconds`: histogram
    - `http_request_duration_summary_seconds`: summary
    - `node_gauge_example`: gauge for tracking async task duration

### Testing Alerts
- To test the alerting system, manually crash the container more than 2 times to trigger an alert (email notification).
- To crash the application container, hit the following endpoint
- `<<LOAD_BALANCER_DNS_NAME>>/crash`
- You should receive an email once the application container has restarted at least 3 times.

---
##  ‚ö†Ô∏è Configure Alertmanager in My system

- First Clone the Alertmanager from official Website with that build using go 
```bash
git clone https://github.com/prometheus/alertmanager.git
cd alertmanager
go build -o alertmanager ./cmd/alertmanager
sudo cp /path/to/alertmanager /usr/local/bin/
sudo chmod +x /usr/local/bin/alertmanager
sudo mkdir -p /etc/alertmanager
sudo mkdir -p /var/lib/alertmanager
```
- You can Remove the github we did
```bash
rm -rf github.com/alertmanager
```
- create a Alert Manager config file `alertmanager.yml` in desired location.
```bash
vim /etc/alertmanager/alertmanager.yml
```
```yml
global:
  smtp_smarthost: 'smtp.gmail.com:587'
  smtp_from: 'System@office.com'
  smtp_auth_username: 'aveshpadaya02@gmail.com'
  smtp_auth_password: '----your-app-password----'

route:
  receiver: 'Testing'

receivers:
- name: 'Testing'
  email_configs:
  - to: 'aveshpadaya0@gmail.com'
```
- Also make the changes in config file of prometheus
```bash
vim prometheus.yml
```
```yml
alerting:
  alertmanagers:
    - static_configs:
        - targets: ['localhost:9093']
```
- Restart and Reload Prometheus
```bash 
sudo systemctl daemon-reload
sudo systemctl restart prometheus
sudo systemctl status prometheus
```
- Create an Alert which can triger when the prometheus started
```bash
vim alert.rules.yml
```
```yml
groups:
- name: startup_alerts
  rules:
  - alert: SystemStartup
    expr: time() - process_start_time_seconds{job="prometheus"} < 300
    for: 1m
    labels:
      severity: critical
    annotations:
      summary: "Prometheus system startup detected"
      description: "Prometheus started less than 5 minutes ago."
```
- Configure alert rule in Prometheus
```bash
vim prometheus.yml
```
```yml
rule_files:
  - "path/to/alert.rules.yml" # change the path
```
- Restart and Check Status of Prometheus
```bash
sudo systemctl daemon-reload
sudo systemctl restart prometheus
sudo systemctl status prometheus
```
- Create a service `alertmanager.service`
```bash
sudo vim  /etc/systemd/system/alertmanager.service
```
```service
[Unit]
Description=Prometheus Alertmanager
After=network.target

[Service]
User=alertmanager
Group=alertmanager
Type=simple
ExecStart=/usr/local/bin/alertmanager \
  --config.file=/etc/alertmanager/alertmanager.yml \
  --storage.path=/var/lib/alertmanager

Restart=on-failure

[Install]
WantedBy=multi-user.target
```
- Enable and Start the Alert Manager
```bash
sudo systemctl daemon-reload
sudo systemctl enable alertmanager
sudo systemctl start alertmanager
```
- check status of Alertmanager
```bash
sudo systemctl status alertmanager
```
- If error plz check the logs 
```bash
sudo journalctl -u alertmanager -f
```
---
# üîç Logging overview
- Logging is crucial in any distributed system, especially in Kubernetes, to monitor application behavior, detect issues, and ensure the smooth functioning of microservices.


## üöÄ Importance:
- **Debugging**: Logs provide critical information when debugging issues in applications.
- **Auditing**: Logs serve as an audit trail, showing what actions were taken and by whom.
- **Performance** Monitoring: Analyzing logs can help identify performance bottlenecks.
- **Security**: Logs help in detecting unauthorized access or malicious activities.

## üõ†Ô∏è Tools Available for Logging in Kubernetes
- üóÇÔ∏è EFK Stack (Elasticsearch, Fluentbit, Kibana)
- üóÇÔ∏è EFK Stack (Elasticsearch, FluentD, Kibana)
- üóÇÔ∏è ELK Stack (Elasticsearch, Logstash, Kibana)
- üìä Promtail + Loki + Grafana

## üì¶ EFK Stack (Elasticsearch, Fluentbit, Kibana)
- EFK is a popular logging stack used to collect, store, and analyze logs in Kubernetes.
- **Elasticsearch**: Stores and indexes log data for easy retrieval.
- **Fluentbit**: A lightweight log forwarder that collects logs from different sources and sends them to Elasticsearch.
- **Kibana**: A visualization tool that allows users to explore and analyze logs stored in Elasticsearch.

# üè† Architecture
![Project Architecture](images/architecture.gif)

## üìù Step-by-Step Setup

### 1) Create IAM Role for Service Account
```bash
eksctl create iamserviceaccount \
    --name ebs-csi-controller-sa \
    --namespace kube-system \
    --cluster observability \
    --role-name AmazonEKS_EBS_CSI_DriverRole \
    --role-only \
    --attach-policy-arn arn:aws:iam::aws:policy/service-role/AmazonEBSCSIDriverPolicy \
    --approve
```
- This command creates an IAM role for the EBS CSI controller.
- IAM role allows EBS CSI controller to interact with AWS resources, specifically for managing EBS volumes in the Kubernetes cluster.
- We will attach the Role with service account

### 2) Retrieve IAM Role ARN
```bash
ARN=$(aws iam get-role --role-name AmazonEKS_EBS_CSI_DriverRole --query 'Role.Arn' --output text)
```
- Command retrieves the ARN of the IAM role created for the EBS CSI controller service account.

### 3) Deploy EBS CSI Driver
```bash
eksctl create addon --cluster observability --name aws-ebs-csi-driver --version latest \
    --service-account-role-arn $ARN --force
```
- Above command deploys the AWS EBS CSI driver as an addon to your Kubernetes cluster.
- It uses the previously created IAM service account role to allow the driver to manage EBS volumes securely.

### 4) Create Namespace for Logging
```bash
kubectl create namespace logging
```

### 5) Install Elasticsearch on K8s

```bash
helm repo add elastic https://helm.elastic.co

helm install elasticsearch \
 --set replicas=1 \
 --set volumeClaimTemplate.storageClassName=gp2 \
 --set persistence.labels.enabled=true elastic/elasticsearch -n logging
```
- Installs Elasticsearch in the `logging` namespace.
- It sets the number of replicas, specifies the storage class, and enables persistence labels to ensure
data is stored on persistent volumes.

### 6) Retrieve Elasticsearch Username & Password
```bash
# for username
kubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.username}' | base64 -d
# for password
kubectl get secrets --namespace=logging elasticsearch-master-credentials -ojsonpath='{.data.password}' | base64 -d
```
- Retrieves the password for the Elasticsearch cluster's master credentials from the Kubernetes secret.
- The password is base64 encoded, so it needs to be decoded before use.
- üëâ **Note**: Please write down the password for future reference

### 7) Install Kibana
```bash
helm install kibana --set service.type=LoadBalancer elastic/kibana -n logging
```
- Kibana provides a user-friendly interface for exploring and visualizing data stored in Elasticsearch.
- It is exposed as a LoadBalancer service, making it accessible from outside the cluster.

### 8) Install Fluentbit with Custom Values/Configurations
- üëâ **Note**: Please update the `HTTP_Passwd` field in the `fluentbit-values.yml` file with the password retrieved earlier in step 6: (i.e NJyO47UqeYBsoaEU)"
```bash
helm repo add fluent https://fluent.github.io/helm-charts
helm install fluent-bit fluent/fluent-bit -f fluentbit-values.yaml -n logging
```
